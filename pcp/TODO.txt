To do list
(this list is not extensive yet, but more of a brain dump to get things rolling)

New architecture specific

- data logging daemon (see datalog.py)
- sweeping implementation 
	- if data is being saved to file continuously, how should we identify packets when sweeping, streaming, nodding…etc. 
	- one simple way would be to have a status.live file which contains the current state of each roach that is updated by the control script, and when each packet is saved by the logger, a flag is saved along with the packet (e.g. time, flag, I0, Q0, I1, Q1…etc). Flag could be a simple string with values: idle, sweep, stream, calibration, sky-nod…etc).
According to Sam this should be pretty easy and he does use "feature bits" on his own

- Readout power optimisation (does that exist anywhere?)
- Sweeping with multiple roaches (simultaneously)
	- this could be achieved with a simple asynchronous multiprocessing function quite easily.

- LMT interfacing (see lmt_interface.py).
	- this (sh)could be borrowed from Sam R’s remote control interface written to interface to Labview.




Logging framework/pseudo-code

# 20180219 - start to bring all the elements of code together. Plan should be to test individual parts of code
# interactively. Then the script will be run using the if __name__ == "__main__" construct at the end of this file

# -- pseudocode --

# initialise from configuration files
# set up process-id lockfile
# configure threading for producer consumer loop
# dirfile generation - when to do this?
#   - this should be handled with the start/stop of data saving
# configure network stuff (separate script?)
# start/stop data saving
# functions to parse data from packet (separate script )


